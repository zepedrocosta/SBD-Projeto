%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                           %
%                            Area for text                                  %
%                                                                           %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:intro}

% Tirei da cena que vcs mandaram para o stor
This project aims to benchmark and compare the performance of different database systems using the TPC-C workload, a standard for evaluating OLTP (Online Transaction Processing) environments.

Automated scripts are used to run identical tests across multiple databases, each configured with similar settings to ensure a fair comparison.

Key metrics such as Transactions Per Minute (TPM) are measured under varying levels of concurrent users.

The results help identify the strengths and limitations of each database in handling transactional workloads.

In this project, the databases used were PostgreSQL, MySQL and MariaDB.

In total, we ran 54 tests:

\begin{itemize}
    \setlength\itemsep{0.1em}
    \item 4 tests scaling the number of virtual users (2 4 8 12) and warehouses (VU*5) on all PCs and databases (48 tests in total);
    \item 1 test with the number of virtual users set to the same as the number of threads in that PC, warehouses set to VU*5, \emph{allwarehouse} = true on all databases on just one PC (3 test in total);
    \item 1 test with the number of virtual users set to the same as the number of threads in that PC, warehouses set to VU*5, with default config on all databases on just one PC (3 test in total).
\end{itemize}

\section{Overview of HammerDB}
\label{sec:hammerdb}

HammerDB is a free, open-source tool for benchmarking the performance of relational databases \cite{enwiki:1275860580}.

It supports popular databases like Oracle, SQL Server, PostgreSQL, MySQL, and more. HammerDB uses industry-standard workloads such as TPROC-C and TPROC-H to simulate real-world database activity.

It offers both a graphical interface and command-line options, making it suitable for developers, DBAs, and system administrators to test, compare, and tune database performance.

In some cases we used HammerDB in docker containers to run the tests, which allows for easy setup and isolation of the testing environment.

In another case, we used the Windows version of HammerDB to run the tests on a Windows machine.

\subsection{Overview of TPROC-C}
\label{sec:tproc-c}

TPROC-C is a benchmark designed to evaluate the performance of database management systems (DBMS) using a transactional workload. It simulates a typical online transaction processing (OLTP) environment, focusing on operations like inserts, updates, and deletes across multiple tables.

\subsection{TPROC-C vs TPROC-H}
\label{sec:tproc-c-vs-tproc-h}

TPROC-H is a benchmark designed for data warehousing and analytical workloads, while TPROC-C is focused on transactional processing. TPROC-H emphasizes complex queries and large data sets, whereas TPROC-C simulates real-time transactions with a focus on insert, update, and delete operations.

\section{Problem \& DBMS Summary}
\label{sec:problem}
Modern applications progressively depend on robust, scalable database systems to effectively manage workloads in a transactional manner. Choosing the right Database Management System (DBMS) is critical to achieve the best possible performance, especially where concurrency is high and the workload is varied. With so many DBMSs to select from, with their own strengths, configurations, and community support, getting the right one can be problematic.

The main objective of this study is to provide a clear comparison of how three of the most popular open-source DBMSs like,PostgreSQL, MySQL, and MariaDB perform under TPROC-C workloads. This is a close approximation of OLTP environments and thus is suitable to use in evaluating systems for high-throughput transaction processing.

A brief overview of the DBMSs under test is provided below:

\begin{itemize}
    \setlength\itemsep{0.1em}
    \item \textbf{PostgreSQL}: It is renowned for support of sophisticated query capabilities, extensibility, and strict support for data integrity features.
    \item \textbf{MySQL}: Extensively used in web development, valued as being easy to use and swift, with excellent ecosystem and support.
    \item \textbf{MariaDB}: A fork of MySQL with an emphasis on improved speed, open development, and additional storage engines.
\end{itemize}

Each DBMS was installed with identical hardware and software configurations to provide a level playing field for the tests. Experiments were designed to highlight differences in how each system handles transaction loads, concurrency, and configuration parameters. Through comparison of performance in a systematic manner across controlled tests, this study aims to guide database selection on the grounds of empirical evidence and not assumptions.


\section{Benchmark Description}
\label{sec:benchmark}

The benchmark used in the research is the TPROC-C workload using HammerDB. TPROC-C is designed to simulate an OLTP environment in the average case and is composed of transactions containing new orders, orders to make payment, checking orders status, delivering orders, and updating stock status. The aforementioned operations can be likened to real-world application environments.

\subsection{Benchmark Goals}

The main objectives of the benchmark are to:

\begin{itemize}
    \setlength\itemsep{0.1em}
    \item Compare the throughputs of the DBMSs at different levels of concurrency in terms of Transactions Per Minute, or TPM.
    \item Monitor the scalability of the systems as virtual users and additional warehouses get created.
    \item Measure consistency across repeated experiments as well as with different setups.
\end{itemize}

\subsection{Test Parameters}

To ensure consistency and fairness, the same configuration template was used for all tests, with the sole variations being the number of virtual users, warehouses, and the specific DBMS being tested. The significant parameters were:

\begin{itemize}
    \setlength\itemsep{0.1em}
    \item \textbf{Virtual Users (VU)}: Simulated clients making postings simultaneously. Applying the values 2, 4, 8, and 12 to perform the scaling test.
    \item \textbf{Warehouses}: A TPROC-C scale unit. Five times the number of virtual users.
    \item \textbf{Test Duration}: Each test was executed in medium one hour.
    \item \textbf{Ramp-Up and Cool-Down Time}: Confirmed with HammerDB setup to allow systems to reach steady state before measurement.
\end{itemize}

\subsection{Execution Environment}

All the testing was automated with custom scripts offering the same setup routines and execution across the systems. The environment included combinations of Windows and Linux systems, based on the setup. HammerDB was run in some cases in Docker containers to offer isolation to the test environment and ensure repeatability.

\subsection{Metrics Collected}

A critical measurement during the benchmarking was the Transaction Per Minute (TPM), as posted by HammerDB. The measurement is indicative of the system capability to process new orders and follow-up transactions in one minute. Secondary measures included CPU usage and memory use.

\subsection{Limitations}

Even though control over variables, consistency across runs, and bias to a variable as limited as possible was the focus of this testing, there are several limitations:

\begin{itemize}
    \setlength\itemsep{0.1em}
    \item Variability in network latency, and relative performance of hardware and storage may be negligible in each of the testing environments.
    \item Unless otherwise stated, default DBMS tuning parameters were used that are not reflective of, nor necessarily represent, the best performance that could be achieved for each database management system.
    \item The intent and focus were on relative performance under a specified workload, not full optimization for each system.
\end{itemize}


\section{Methodology}
\label{sec:methodology}

\subsection{Hardware and Software Setup}
\label{sec:hardware-software-setup}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{PC}        & \textbf{1}       & \textbf{2}      & \textbf{3}      & \textbf{4}    \\
        \hline
        \textbf{OS}        & Windows 11       & Windows 11      & Linux (Unraid)  & MacOS Sequoia \\
        \hline
        \textbf{CPU}       & AMD Ryzen 5 3600 & Intel i7-13700H & Intel i3-10100F & Apple M1      \\
        \hline
        \textbf{Cores}     & 6                & 14 (6P 8E)      & 4               & 8             \\
        \hline
        \textbf{Threads}   & 12               & 20              & 8               & 8             \\
        \hline
        \textbf{RAM}       & 16GB             & 16GB            & 32GB            & 16GB          \\
        \hline
        \textbf{Disk}      & SSD M.2 NVMe     & SSD M.2 NVMe    & SSD M.2 NVMe    & SSD M.2 NVMe  \\
        \hline
        \textbf{Read}      & 2500 MB/s        & 3500 MB/s       & 3500 MB/s       & 3400 MB/s     \\
        \hline
        \textbf{Write}     & 2100 MB/s        & 2700 MB/s       & 3300 MB/s       & 2800 MB/s     \\
        \hline
        \textbf{Test type} & Bare metal       & Docker          & Docker          & Docker        \\
        \hline
    \end{tabular}
    \caption{Hardware used in the benchmarks}
    \label{tab:hardware-setup}
\end{table}

The benchmarking tests were conducted on four various personal computers, each of which satisfied the same specification for the purpose of ongoing performance measurement.

\begin{itemize}
    \setlength\itemsep{0.1em}
    \item \textbf{PC 1:} All the components, such as HammerDB and the database systems, were installed natively.
    on the native operating system (bare metal). The reason for this installation was to test performance without the overhead of running in a container.
    
    \item \textbf{PCs 2, 3, and 4:} HammerDB and each DBMS were run within Docker containers. This approach added an audio system that facilitated differentiation between services and guaranteed uniformity across devices.
\end{itemize}

We utilized \hyperref[sec:docker-compose]{Docker Compose} as our orchestration tool, allowing the deployment and usage of the same setup as configured in multiple machines with minimal processing. 

For \hyperref[sec:dockerfile-mysql]{MySQL} and \hyperref[sec:dockerfile-mariadb]{MariaDB} alone, we developed tailored Dockerfiles that would facilitate incorporating external configuration files. This is due to the fact that the databases defaulted to read-only access to the configuration file, unlike PostgreSQL, within the container.

\subsection{Database Setup}
\label{sec:database-setup}

To prevent inconsistency and promote fairness in benchmarking, MySQL, MariaDB, and PostgreSQL were all installed along with their respective configuration files to define the pertinent operating configurations needed in the benchmarks. Configurations were rendered as consistent as possible on systems, taking into consideration the individual tunables and limitations of each DBMS.

\begin{itemize}
    \setlength\itemsep{0.1em}
    \item \textbf{MySQL:} The configuration for MySQL is found in Appendix \hyperref[sec:mysql-config]{MySQL}. It has been optimized for transactional workloads including parameters such as \texttt{innodb\_buffer\_pool\_size} and \texttt{innodb\_log\_file\_size}.
    \item \textbf{MariaDB:} The MariaDB configuration, described in Appendix \hyperref[sec:mariadb-config]{MariaDB}, was tailored to the MySQL configuration to make best use of the MariaDB tuning parameters and behaviour of the storage engines.
    \item \textbf{PostgreSQL:} PostgreSQL was configured as described in Appendix \hyperref[sec:postgresql-config]{PostgreSQL}. Performance-related parameters discussed were standard to its use like \texttt{shared\_buffers}, \texttt{work\_mem} and \texttt{max\_connections} adjusted for the hardware limits and benchmark requirements.
\end{itemize}

Custom Dockerfiles were created for MySQL and MariaDB in order to mount and execute the external configuration files at the time of container initialization, thereby overriding the default read-only restriction on their internal configuration files. PostgreSQL offers a simpler method of configuration alteration but permits its internal configuration files to be altered using volume mounting and environment variables. 

Each database instance was restarted with a new schema for each test iteration to eliminate residual data artifacts and ensure consistency. Loading a schema and provisioning a user were also scripted to minimize the level of manual intervention and preserve reproducibility between testing cycles.

\section{Results}
\label{sec:results}

\section{Discussion}
\label{sec:discussion}

\section{Conclusions}
\label{sec:conclusions}
